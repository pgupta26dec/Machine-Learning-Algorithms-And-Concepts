{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GNgYI3H4P5by",
        "djByijaPSNuK",
        "wbxDACVWTd6U"
      ],
      "authorship_tag": "ABX9TyOiqqTj44PHnXxsXzBi00va",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pgupta26dec/Machine-Learning-Algorithms-And-Concepts/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines"
      ],
      "metadata": {
        "id": "a865m1BiPdqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Support Vector Machine?"
      ],
      "metadata": {
        "id": "GNgYI3H4P5by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Support Vector Machine is a supervised machine learning algorithm that can be used for both regression and classification.\n",
        "* The basic idea behind SVM is that it is focused on finding a hyperplane(or a threshold) that best divides the dataset into classes.\n"
      ],
      "metadata": {
        "id": "gQaL6V-PSMYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terminologies for SVM"
      ],
      "metadata": {
        "id": "djByijaPSNuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   ***Hyperplane*** : A hyperplane is a decision plane which separates between a set of objects having different class memberships.\n",
        "*   ***Margin*** : A margin is a gap between the two lines on the closest class points.\n",
        "* ***Soft Margin*** : When we allow misclassifications, the distance between the threshold and the closest data points is called the soft margin.\n",
        "* ***Support Vectors*** : Support vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n",
        "* ***Maximum Margin Classifier*** : The classifier which uses a hyperplane such that the margin is highest. (in 1D data setting, when the hyperplane is exactly in the middle of the support vectors, the margin is highest)\n",
        "* ***Soft Margin Classifier*** : When we use a soft margin to determine the location of the threshold, the classifier is called soft margin classifier aka support vector classifier.(the name comes from the fact that the observations on the edge and within the soft margin are called support vectors)\n",
        "\n"
      ],
      "metadata": {
        "id": "GeUJHp3BSQbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common questions for SVM"
      ],
      "metadata": {
        "id": "wbxDACVWTd6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Is Maximum Margin Classifier the best?<br>\n",
        "No. It seems cool but it will be heavily affected in the case of outliers in the training data. \n",
        "\n",
        "2.   How to make a threshold that is not sensitive to outliers?<br>\n",
        "To do this, we have to allow misclassifications in the training data. This allows us to build a classifier that is general and does not overfits the training data. Allowing misclassifications while building the classifier is an example of 'bias-variance tradeoff'\n",
        "\n",
        "3. How to we know which soft margin to use, i.e. how many outliers within a soft margin to come up with the best classifier?<br>\n",
        "Cross Validation to determine how many misclassifications and observations within a soft margin would make the best classifier.\n",
        "\n",
        "4. How does support vector classifier deal with different dimensions of data?<br>\n",
        "  *   When the data is one dimensional, the classifier is a point(a point is a flat affine 0 dimensional subspace).\n",
        "  *   When the data is two dimensional, support vector classification is a line(a line is a flat affine 1 dimensional subspace).\n",
        "  *   When the data is three dimensional, support vector classifier forms a plane instead of a line(a plane is a flat affine 2 dimensional subspace).\n",
        "  *   When the data is four dimensional, support vector classifier is a hyperplane(a hyperplane is a flat affine subspace).\n",
        "  * Technically, all flat affine subspaces are hyperplanes, but generally used for four dimesional data.\n",
        "\n",
        "5. What are the different kernels in Support Vector Machine?\n",
        "  *   Polynomial Kernel<br> The polynomial kernel has parameter d, which stands for degree of the polynomial.<br> When d = 1, the polynomial kernel computes the relationships between each pair of observations in 1-Dimension. These relationships are used to find svc<br> When d=2, the polunomial kernel computes 2d relationships between observations and so on..<br> **The best value of d can be found using cross validation.**\n",
        "  *   Radial Basis Function Kernel<br>The radial kernel finds support vector classifiers in infinite dimensions.<br>The radial kernel behaves like a weighted nearest neighbor model, i.e., the  closest observations(the nearest neighbors) have a lot of influence on how the new observation is classified.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ez3YWT5DTh54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intuition behind Support Vector Machine?"
      ],
      "metadata": {
        "id": "GMuZIgDbP-Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Start with data in a relatively low dimension.\n",
        "2.   Move the data into a higher dimension.\n",
        "3.   Find a support vector classifier that separates the data into two groups.\n",
        "\n",
        "------\n",
        "\n",
        "\n",
        "\n",
        "*   Support Vector Machines use Kernel functions to systematically find support vector classifiers in higher dimensions\n",
        "*   Kernel functions only calculate the relationships between each pair of points as if they are in higher dimensions; ***they do not actually do the transformation***. This trick of calculating high-dimensional relationships without actually transforming the data to higher dimensions is called the kernel trick. This trick reduces the amount of computation by avoiding the math. It makes radial kernel possible.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DwhGRPw3Zqnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters for SVM"
      ],
      "metadata": {
        "id": "5Us4H-z6jB3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Kernel**: <br>The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF). Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear. This transformation can lead to more accurate classifiers.\n",
        "*   **Regularization**: <br>Regularization parameter in python's Scikit-learn C parameter used to maintain regularization. Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n",
        "*  **Gamma**: <br>A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting. In other words, you can say a low value of gamma considers only nearby points in calculating the separation line, while the a value of gamma considers all the data points in the calculation of the separation line.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ygn3iqbXjH9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM classifier using Scikit Learn"
      ],
      "metadata": {
        "id": "LMs3Vsn9hiwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "#Load dataset\n",
        "cancer = datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "QBMB4RBuhmNl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE5_M4Z1htqC",
        "outputId": "3afb2570-7ba1-4fca-eb66-e6601d79d498"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109)"
      ],
      "metadata": {
        "id": "rIFlOEREhyVJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import svm model\n",
        "from sklearn import svm\n",
        "\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "\n",
        "#Train the model using the training sets\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "XseG6pkXh4cH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e80d-Q9Qh_yv",
        "outputId": "4e613828-fec9-4472-9188-788d420a690f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBZCvhmGiB87",
        "outputId": "d33d9d0a-e62e-4547-a4b7-83fa85a03c62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9811320754716981\n",
            "Recall: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Regression using scikit learn"
      ],
      "metadata": {
        "id": "PXn1z3VfrKMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U_3ZcV6FiD5G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}